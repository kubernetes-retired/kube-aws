#cloud-config
coreos:
  update:
    reboot-strategy: "off"
  flannel:
    interface: $private_ipv4
    etcd_cafile: /etc/kubernetes/ssl/ca.pem
    etcd_certfile: /etc/kubernetes/ssl/etcd-client.pem
    etcd_keyfile: /etc/kubernetes/ssl/etcd-client-key.pem

  units:
    - name: cfn-etcd-environment.service
      enable: true
      command: start
      runtime: true
      content: |
        [Unit]
        Description=Fetches etcd static IP addresses list from CF
        After=network-online.target

        [Service]
        EnvironmentFile={{.StackNameEnvFileName}}
        Restart=on-failure
        RemainAfterExit=true
        ExecStartPre=/opt/bin/cfn-etcd-environment
        ExecStart=/usr/bin/mv -f /var/run/coreos/etcd-environment /etc/etcd-environment

    - name: docker.service
      drop-ins:
{{if .Experimental.EphemeralImageStorage.Enabled}}
        - name: 10-docker-mount.conf
          content: |
            [Unit]
            After=var-lib-docker.mount
            Wants=var-lib-docker.mount
{{end}}
        - name: 40-flannel.conf
          content: |
            [Unit]
            Wants=flanneld.service
            [Service]
            EnvironmentFile=/etc/kubernetes/cni/docker_opts_cni.env
            ExecStartPre=/usr/bin/systemctl is-active flanneld.service

        - name: 60-logfilelimit.conf
          content: |
            [Service]
            Environment="DOCKER_OPTS=--log-opt max-size=50m --log-opt max-file=3"

    - name: flanneld.service
      drop-ins:
        - name: 10-etcd.conf
          content: |
            [Unit]
            Wants=cfn-etcd-environment.service
            After=cfn-etcd-environment.service

            [Service]
            EnvironmentFile=-/etc/etcd-environment
            EnvironmentFile=-/run/flannel/etcd-endpoints.opts
            ExecStartPre=/usr/bin/systemctl is-active cfn-etcd-environment.service
            ExecStartPre=/bin/sh -ec "echo FLANNELD_ETCD_ENDPOINTS=${ETCD_ENDPOINTS} >/run/flannel/etcd-endpoints.opts"
            ExecStartPre=/opt/bin/decrypt-tls-assets
            Environment="ETCD_SSL_DIR=/etc/kubernetes/ssl"
            TimeoutStartSec=120

    - name: kubelet.service
      command: start
      runtime: true
      content: |
        [Unit]
        Wants=flanneld.service cfn-etcd-environment.service
        After=cfn-etcd-environment.service
        [Service]
        EnvironmentFile=-/etc/etcd-environment
        Environment=KUBELET_VERSION={{.K8sVer}}
        Environment=KUBELET_ACI={{.HyperkubeImageRepo}}
        Environment="RKT_OPTS=--volume dns,kind=host,source=/etc/resolv.conf \
        --set-env=ETCD_CA_CERT_FILE=/etc/kubernetes/ssl/ca.pem \
        --set-env=ETCD_CERT_FILE=/etc/kubernetes/ssl/etcd-client.pem \
        --set-env=ETCD_KEY_FILE=/etc/kubernetes/ssl/etcd-client-key.pem \
        --mount volume=dns,target=/etc/resolv.conf \
        --volume rkt,kind=host,source=/opt/bin/host-rkt \
        --mount volume=rkt,target=/usr/bin/rkt \
        --volume var-lib-rkt,kind=host,source=/var/lib/rkt \
        --mount volume=var-lib-rkt,target=/var/lib/rkt \
        --volume var-lib-cni,kind=host,source=/var/lib/cni \
        --mount volume=var-lib-cni,target=/var/lib/cni \
        --volume stage,kind=host,source=/tmp \
        --mount volume=stage,target=/tmp \
        --volume var-log,kind=host,source=/var/log \
        --mount volume=var-log,target=/var/log{{ if .UseCalico }} \
        --volume cni-bin,kind=host,source=/opt/cni/bin \
        --mount volume=cni-bin,target=/opt/cni/bin{{ end }}"
        ExecStartPre=/usr/bin/systemctl is-active flanneld.service
        ExecStartPre=/usr/bin/systemctl is-active cfn-etcd-environment.service
        ExecStartPre=/usr/bin/mkdir -p /var/lib/cni
        ExecStartPre=/usr/bin/mkdir -p /var/log/containers
        ExecStartPre=/usr/bin/mkdir -p /opt/cni/bin
        ExecStartPre=/bin/sh -ec "find /etc/kubernetes/manifests /etc/kubernetes/cni/net.d/  -maxdepth 1 -type f | xargs --no-run-if-empty sed -i 's|#ETCD_ENDPOINTS#|${ETCD_ENDPOINTS}|'"
        ExecStartPre=/usr/bin/etcdctl \
                       --ca-file /etc/kubernetes/ssl/ca.pem \
                       --key-file /etc/kubernetes/ssl/etcd-client-key.pem \
                       --cert-file /etc/kubernetes/ssl/etcd-client.pem \
                       --endpoints "${ETCD_ENDPOINTS}" \
                       cluster-health
        ExecStart=/usr/lib/coreos/kubelet-wrapper \
        --api-servers={{.APIServerEndpoint}} \
        --network-plugin-dir=/etc/kubernetes/cni/net.d \
        --network-plugin={{.K8sNetworkPlugin}} \
        --container-runtime={{.ContainerRuntime}} \
        --rkt-path=/usr/bin/rkt \
        --rkt-stage1-image=coreos.com/rkt/stage1-coreos \
        {{if .Experimental.NodeLabels.Enabled}}--node-labels {{.Experimental.NodeLabels.String}} \
        {{end}}--register-node=true \
        {{if .Experimental.Taints}}--register-schedulable=false \
        {{end}}--allow-privileged=true \
        --pod-manifest-path=/etc/kubernetes/manifests \
        --cluster_dns={{.DNSServiceIP}} \
        --cluster_domain=cluster.local \
        --cloud-provider=aws \
        --kubeconfig=/etc/kubernetes/worker-kubeconfig.yaml \
        --tls-cert-file=/etc/kubernetes/ssl/worker.pem \
        --tls-private-key-file=/etc/kubernetes/ssl/worker-key.pem
        Restart=always
        RestartSec=10
        [Install]
        WantedBy=multi-user.target

{{ if eq .ContainerRuntime "rkt" }}
    - name: rkt-api.service
      enable: true
      content: |
        [Unit]
        Before=kubelet.service
        [Service]
        ExecStart=/usr/bin/rkt api-service
        Restart=always
        RestartSec=10
        [Install]
        RequiredBy=kubelet.service

    - name: load-rkt-stage1.service
      enable: true
      content: |
        [Unit]
        Description=Load rkt stage1 images
        Documentation=http://github.com/coreos/rkt
        Requires=network-online.target
        After=network-online.target
        Before=rkt-api.service
        [Service]
        Type=oneshot
        RemainAfterExit=yes
        ExecStart=/usr/bin/rkt fetch /usr/lib/rkt/stage1-images/stage1-coreos.aci /usr/lib/rkt/stage1-images/stage1-fly.aci  --insecure-options=image
        [Install]
        RequiredBy=rkt-api.service
{{ end }}

{{ if .NodeDrainer.Enabled }}
    - name: kube-node-drainer.service
      enable: true
      command: start
      runtime: true
      content: |
        [Unit]
        Description=drain this k8s node to make running pods time to gracefully shut down before stopping kubelet
        After=multi-user.target

        [Service]
        Type=oneshot
        RemainAfterExit=true
        ExecStart=/bin/true
        TimeoutStopSec=30s
        ExecStop=/bin/sh -c '/usr/bin/rkt run \
        --volume=kube,kind=host,source=/etc/kubernetes,readOnly=true \
        --mount=volume=kube,target=/etc/kubernetes \
        --net=host \
        {{.HyperkubeImageRepo}}:{{.K8sVer}} \
          --exec=/kubectl -- \
          --server=https://{{.ExternalDNSName}}:443 \
          --kubeconfig=/etc/kubernetes/worker-kubeconfig.yaml \
          drain $(hostname) \
          --ignore-daemonsets \
          --force'

        [Install]
        WantedBy=multi-user.target
{{ end }}

{{if .AwsEnvironment.Enabled}}
    - name: set-aws-environment.service
      enable: true
      command: start
      runtime: true
      content: |
        [Unit]
        Description=Set AWS environment variables in /etc/aws-environment
        After=network-online.target

        [Service]
        Type=oneshot
        EnvironmentFile={{.StackNameEnvFileName}}
        RemainAfterExit=true
        ExecStartPre=/bin/touch /etc/aws-environment
        ExecStart=/opt/bin/set-aws-environment
{{end}}

{{if .SpotFleet.Enabled}}
    - name: tag-spot-instance.service
      enable: true
      command: start
      runtime: true
      content: |
        [Unit]
        Description=Tag this spot instance with cluster name
        After=network-online.target

        [Service]
        Type=oneshot
        RemainAfterExit=true
        ExecStart=/opt/bin/tag-spot-instance

{{if .LoadBalancer.Enabled}}
    - name: add-to-load-balancers.service
      enable: true
      command: start
      runtime: true
      content: |
        [Unit]
        Description=Add this spot instance to load balancers
        After=network-online.target

        [Service]
        Type=oneshot
        RemainAfterExit=true
        ExecStart=/opt/bin/add-to-load-balancers
{{end}}
{{end}}

{{ if $.ElasticFileSystemID }}
    - name: rpc-statd.service
      command: start
      enable: true
    - name: efs.service
      command: start
      content: |
        [Unit]
        After=network-online.target
        [Service]
        Type=oneshot
        ExecStartPre=-/usr/bin/mkdir -p /efs
        ExecStart=/bin/sh -c 'grep -qs /efs /proc/mounts || /usr/bin/mount -t nfs4 -o nfsvers=4.1,rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2 $(/usr/bin/curl -s http://169.254.169.254/latest/meta-data/placement/availability-zone).{{ $.ElasticFileSystemID }}.efs.{{ $.Region }}.amazonaws.com:/ /efs'
        ExecStop=/usr/bin/umount /efs
        RemainAfterExit=yes
        [Install]
        WantedBy=kubelet.service
{{ end }}

{{if .Experimental.Taints }}
    - name: kube-node-taint-and-uncordon.service
      command: start
      runtime: true
      content: |
        [Unit]
        Description=Taint this kubernetes node with user-provided taints and then uncordon it
        Wants=kubelet.service
        After=kubelet.service
        Before=cfn-signal.service

        [Service]
        Type=simple
        StartLimitInterval=0
        RestartSec=10
        Restart=on-failure
        ExecStartPre=/usr/bin/systemctl is-active kubelet.service
        ExecStartPre=/usr/bin/bash -c "while sleep 1; do if /usr/bin/curl  --insecure -s -m 20 -f  https://127.0.0.1:10250/healthz > /dev/null ; then break ; fi;  done"
        ExecStart=/opt/bin/taint-and-uncordon
{{end}}

{{ if .WaitSignal.Enabled }}
    - name: cfn-signal.service
      command: start
      content: |
        [Unit]
        Wants=kubelet.service docker.service
        After=kubelet.service

        [Service]
        Type=oneshot
        EnvironmentFile={{.StackNameEnvFileName}}
        ExecStartPre=/usr/bin/bash -c "while sleep 1; do if /usr/bin/curl  --insecure -s -m 20 -f  https://127.0.0.1:10250/healthz > /dev/null ; then break ; fi;  done"
        {{ if .UseCalico }}
        ExecStartPre=/usr/bin/bash -c "until /usr/bin/docker run --net=host --pid=host --rm calico/ctl:v1.0.0 node status > /dev/null; do sleep 3; done && echo Calico running"        
        {{ end }}
        ExecStart=/opt/bin/cfn-signal
{{end}}

{{if .Experimental.AwsNodeLabels.Enabled }}
    - name: kube-node-label.service
      enable: true
      command: start
      runtime: true
      content: |
        [Unit]
        Description=Label this kubernetes node with additional AWS parameters
        After=kubelet.service
        Before=cfn-signal.service

        [Service]
        Type=oneshot
        RemainAfterExit=true
        ExecStop=/bin/true
        ExecStartPre=/bin/sh -c "/usr/bin/systemctl set-environment INSTANCE_ID=$(/usr/bin/curl -s http://169.254.169.254/latest/meta-data/instance-id)"
        ExecStartPre=/bin/sh -c "/usr/bin/systemctl set-environment SECURITY_GROUPS=\"$(/usr/bin/curl -s http://169.254.169.254/latest/meta-data/security-groups | tr '\n' ',')\""
        ExecStartPre=/bin/sh -c "/usr/bin/systemctl set-environment AUTOSCALINGGROUP=\"$(/usr/bin/docker run --rm --net=host \
          {{.AWSCliImageRepo}}:{{.AWSCliTag}} aws \
          autoscaling describe-auto-scaling-instances \
          --instance-ids ${INSTANCE_ID} --region {{.Region}} \
          --query 'AutoScalingInstances[].AutoScalingGroupName' --output text)\""
        ExecStartPre=/bin/sh -c "/usr/bin/systemctl set-environment \
          LAUNCHCONFIGURATION=\"$(/usr/bin/docker run --rm --net=host \
          {{.AWSCliImageRepo}}:{{.AWSCliTag}} \
          aws autoscaling describe-auto-scaling-groups \
          --auto-scaling-group-name $AUTOSCALINGGROUP --region {{.Region}} \
          --query 'AutoScalingGroups[].LaunchConfigurationName' --output text)\""
        ExecStart=/bin/sh -c "/usr/bin/curl \
          --cert   /etc/kubernetes/ssl/worker.pem \
          --key    /etc/kubernetes/ssl/worker-key.pem \
          --cacert /etc/kubernetes/ssl/ca.pem  \
          --request PATCH \
          -H 'Content-Type: application/strategic-merge-patch+json' \
          -d'{ \
          \"metadata\": { \
            \"labels\": { \
              \"kube-aws.coreos.com/autoscalinggroup\": \"${AUTOSCALINGGROUP}\", \
              \"kube-aws.coreos.com/launchconfiguration\": \"${LAUNCHCONFIGURATION}\" \
            }, \
            \"annotations\": { \
              \"kube-aws.coreos.com/securitygroups\": \"${SECURITY_GROUPS}\" \
            } \
          } \
          }\"' \
          https://{{.ExternalDNSName}}:443/api/v1/nodes/$(hostname)"
{{end}}

{{if .Experimental.EphemeralImageStorage.Enabled}}
    - name: format-ephemeral.service
      command: start
      content: |
        [Unit]
        Description=Formats the ephemeral drive
        ConditionFirstBoot=yes
        After=dev-{{.Experimental.EphemeralImageStorage.Disk}}.device
        Requires=dev-{{.Experimental.EphemeralImageStorage.Disk}}.device
        [Service]
        Type=oneshot
        RemainAfterExit=yes
        ExecStart=/usr/sbin/wipefs -f /dev/{{.Experimental.EphemeralImageStorage.Disk}}
        ExecStart=/usr/sbin/mkfs.{{.Experimental.EphemeralImageStorage.Filesystem}} -f /dev/{{.Experimental.EphemeralImageStorage.Disk}}
    - name: var-lib-docker.mount
      command: start
      content: |
        [Unit]
        Description=Mount ephemeral to /var/lib/docker
        Requires=format-ephemeral.service
        After=format-ephemeral.service
        [Mount]
        What=/dev/{{.Experimental.EphemeralImageStorage.Disk}}
{{if eq .ContainerRuntime "docker"}}
        Where=/var/lib/docker
{{else if eq .ContainerRuntime "rkt"}}
        Where=/var/lib/rkt
{{end}}
        Type={{.Experimental.EphemeralImageStorage.Filesystem}}
{{end}}

{{if .SSHAuthorizedKeys}}
ssh_authorized_keys:
  {{range $sshkey := .SSHAuthorizedKeys}}
  - {{$sshkey}}
  {{end}}
{{end}}

write_files:
{{if .AwsEnvironment.Enabled}}
  - path: /opt/bin/set-aws-environment
    owner: root:root
    permissions: 0700
    content: |
      #!/bin/bash -e

      rkt run \
        --volume=dns,kind=host,source=/etc/resolv.conf,readOnly=true \
        --mount volume=dns,target=/etc/resolv.conf \
        --volume=awsenv,kind=host,source=/etc/aws-environment,readOnly=false \
        --mount volume=awsenv,target=/etc/aws-environment \
        --uuid-file-save=/var/run/coreos/set-aws-environment.uuid \
        --net=host \
        --trust-keys-from-https \
        {{.AWSCliImageRepo}}:{{.AWSCliTag}} --exec=/bin/bash -- \
          -ec \
          '
            cfn-init -v -c "aws-environment" --region {{.Region}} --resource {{.LogicalName}} --stack '${{.StackNameEnvVarName}}'
          '
{{end}}

  - path: /opt/bin/cfn-signal
    owner: root:root
    permissions: 0700
    content: |
      #!/bin/bash -e

      rkt run \
        --volume=dns,kind=host,source=/etc/resolv.conf,readOnly=true \
        --mount volume=dns,target=/etc/resolv.conf \
        --volume=awsenv,kind=host,source=/var/run/coreos,readOnly=false \
        --mount volume=awsenv,target=/var/run/coreos \
        --uuid-file-save=/var/run/coreos/cfn-signal.uuid \
        --net=host \
        --trust-keys-from-https \
        {{.AWSCliImageRepo}}:{{.AWSCliTag}} --exec=/bin/bash -- \
          -ec \
          '
            cfn-signal -e 0 --region {{.Region}} --resource {{.LogicalName}} --stack '${{.StackNameEnvVarName}}'
          '

      rkt rm --uuid-file=/var/run/coreos/cfn-signal.uuid || :

  - path: /opt/bin/cfn-etcd-environment
    owner: root:root
    permissions: 0700
    content: |
      #!/bin/bash -e

      rkt run \
        --volume=dns,kind=host,source=/etc/resolv.conf,readOnly=true \
        --mount volume=dns,target=/etc/resolv.conf \
        --volume=awsenv,kind=host,source=/var/run/coreos,readOnly=false \
        --mount volume=awsenv,target=/var/run/coreos \
        --uuid-file-save=/var/run/coreos/cfn-etcd-environment.uuid \
        --net=host \
        --trust-keys-from-https \
        {{.AWSCliImageRepo}}:{{.AWSCliTag}} --exec=/bin/bash -- \
          -ec \
          '
            cfn-init -v -c "etcd-client" --region {{.Region}} --resource {{.LogicalName}} --stack '${{.StackNameEnvVarName}}'
          '

      rkt rm --uuid-file=/var/run/coreos/cfn-etcd-environment.uuid || :

  - path: /etc/kubernetes/cni/docker_opts_cni.env
    content: |
      DOCKER_OPT_BIP=""
      DOCKER_OPT_IPMASQ=""

  - path: /opt/bin/host-rkt
    permissions: 0755
    owner: root:root
    content: |
      #!/bin/sh
      # This is bind mounted into the kubelet rootfs and all rkt shell-outs go
      # through this rkt wrapper. It essentially enters the host mount namespace
      # (which it is already in) only for the purpose of breaking out of the chroot
      # before calling rkt. It makes things like rkt gc work and avoids bind mounting
      # in certain rkt filesystem dependancies into the kubelet rootfs. This can
      # eventually be obviated when the write-api stuff gets upstream and rkt gc is
      # through the api-server. Related issue:
      # https://github.com/coreos/rkt/issues/2878
      exec nsenter -m -u -i -n -p -t 1 -- /usr/bin/rkt "$@"

{{ if .ManageCertificates }}

  - path: /etc/kubernetes/ssl/etcd-client.pem.enc
    encoding: gzip+base64
    content: {{.TLSConfig.EtcdClientCert}}

  - path: /etc/kubernetes/ssl/etcd-client-key.pem.enc
    encoding: gzip+base64
    content: {{.TLSConfig.EtcdClientKey}}

  - path: /etc/kubernetes/ssl/worker.pem.enc
    encoding: gzip+base64
    content: {{.TLSConfig.WorkerCert}}

  - path: /etc/kubernetes/ssl/worker-key.pem.enc
    encoding: gzip+base64
    content: {{.TLSConfig.WorkerKey}}

  - path: /etc/kubernetes/ssl/ca.pem.enc
    encoding: gzip+base64
    content: {{.TLSConfig.CACert}}

{{ end }}

  - path: /opt/bin/decrypt-tls-assets
    owner: root:root
    permissions: 0700
    content: |
      #!/bin/bash -e

      rkt run \
        --volume=ssl,kind=host,source=/etc/kubernetes/ssl,readOnly=false \
        --mount=volume=ssl,target=/etc/kubernetes/ssl \
        --uuid-file-save=/var/run/coreos/decrypt-tls-assets.uuid \
        --volume=dns,kind=host,source=/etc/resolv.conf,readOnly=true --mount volume=dns,target=/etc/resolv.conf \
        --net=host \
        --trust-keys-from-https \
        {{.AWSCliImageRepo}}:{{.AWSCliTag}} --exec=/bin/bash -- \
          -ec \
          'echo decrypting tls assets
           shopt -s nullglob
           for encKey in /etc/kubernetes/ssl/*.pem.enc; do
             echo decrypting $encKey
             f=$(mktemp $encKey.XXXXXXXX)
             /usr/bin/aws \
               --region {{.Region}} kms decrypt \
               --ciphertext-blob fileb://$encKey \
               --output text \
               --query Plaintext \
             | base64 -d > $f
             mv -f $f ${encKey%.enc}
           done;
           echo done.'

      rkt rm --uuid-file=/var/run/coreos/decrypt-tls-assets.uuid || :

{{if .SpotFleet.Enabled}}
  - path: /opt/bin/tag-spot-instance
    owner: root:root
    permissions: 0700
    content: |
      #!/bin/bash -e

      instance_id=$(curl http://169.254.169.254/latest/meta-data/instance-id)

      sudo rkt run \
        --volume=ssl,kind=host,source=/etc/kubernetes/ssl,readOnly=false \
        --mount=volume=ssl,target=/etc/kubernetes/ssl \
        --uuid-file-save=/var/run/coreos/tag-spot-instance.uuid \
        --volume=dns,kind=host,source=/etc/resolv.conf,readOnly=true --mount volume=dns,target=/etc/resolv.conf \
        --net=host \
        --trust-keys-from-https \
        --insecure-options=ondisk \
        {{.AWSCliImageRepo}}:{{.AWSCliTag}} --exec=/bin/bash -- \
          -vxec \
          'echo tagging this spot instance
           instance_id="'$instance_id'"
           /usr/bin/aws \
             --region {{.Region}} ec2 create-tags \
             --resource $instance_id \
             --tags '"'"'Key=KubernetesCluster,Value="{{.ClusterName}}"'"'"' '"'"'Key=Name,Value="{{.ClusterName}}-{{.StackName}}-kube-aws-worker"'"'"' '"'"'Key="kube-aws:node-pool:name",Value="{{.NodePoolName}}"'"'"'
           echo done.'

      sudo rkt rm --uuid-file=/var/run/coreos/tag-spot-instance.uuid
{{if .Experimental.LoadBalancer.Enabled}}
  - path: /opt/bin/add-to-load-balancers
    owner: root:root
    permissions: 0700
    content: |
      #!/bin/bash -e

      instance_id=$(curl http://169.254.169.254/latest/meta-data/instance-id)

      sudo rkt run \
        --volume=ssl,kind=host,source=/etc/kubernetes/ssl,readOnly=false \
        --mount=volume=ssl,target=/etc/kubernetes/ssl \
        --uuid-file-save=/var/run/coreos/add-to-load-balancers.uuid \
        --volume=dns,kind=host,source=/etc/resolv.conf,readOnly=true --mount volume=dns,target=/etc/resolv.conf \
        --net=host \
        --trust-keys-from-https \
        --insecure-options=ondisk \
        {{.AWSCliImageRepo}}:{{.AWSCliTag}} --exec=/bin/bash -- \
          -vxec \
          'echo adding this spot instance to load balancers
           instance_id="'$instance_id'"
           lbs=({{range $i, $lb := .Experimental.LoadBalancer.Names}}"{{$lb}}" {{end}})
           add_to_lb="/usr/bin/aws --region {{.Region}} elb register-instances-with-load-balancer --instances $instance_id --load-balancer-name"
           for lb in ${lbs[@]}; do
             echo "$lb"
             $add_to_lb "$lb"
           done
           echo done.'

      sudo rkt rm --uuid-file=/var/run/coreos/add-to-load-balancers.uuid
{{end}}
{{end}}

  - path: /opt/bin/taint-and-uncordon
    owner: root:root
    permissions: 0700
    content: |
      #!/bin/bash -e

      hostname=$(hostname)

      docker run --rm --net=host \
        -v /etc/kubernetes:/etc/kubernetes \
        -v /etc/resolv.conf:/etc/resolv.conf \
        {{.HyperkubeImageRepo}}:{{.K8sVer}} /bin/bash \
          -vxec \
          'echo "tainting this node."
           hostname="'${hostname}'"
           taints=({{range $i, $taint := .Experimental.Taints}}"{{$taint.String}}" {{end}})
           kubectl="/kubectl --server=https://{{.ExternalDNSName}}:443 --kubeconfig=/etc/kubernetes/worker-kubeconfig.yaml"
           taint="$kubectl taint node --overwrite"
           for t in ${taints[@]}; do
             $taint "$hostname" "$t"
           done
           echo "done."
           echo "uncordoning this node."
           $kubectl uncordon $hostname
           echo "done."'

  - path: /etc/kubernetes/manifests/kube-proxy.yaml
    content: |
        apiVersion: v1
        kind: Pod
        metadata:
          name: kube-proxy
          namespace: kube-system
          annotations:
            rkt.alpha.kubernetes.io/stage1-name-override: coreos.com/rkt/stage1-fly
        spec:
          hostNetwork: true
          containers:
          - name: kube-proxy
            image: {{.HyperkubeImageRepo}}:{{.K8sVer}}
            command:
            - /hyperkube
            - proxy
            - --master={{.APIServerEndpoint}}
            - --kubeconfig=/etc/kubernetes/worker-kubeconfig.yaml
            securityContext:
              privileged: true
            volumeMounts:
              - mountPath: /etc/ssl/certs
                name: "ssl-certs"
              - mountPath: /etc/kubernetes/worker-kubeconfig.yaml
                name: "kubeconfig"
                readOnly: true
              - mountPath: /etc/kubernetes/ssl
                name: "etc-kube-ssl"
                readOnly: true
              - mountPath: /var/run/dbus
                name: dbus
                readOnly: false
          volumes:
            - name: "ssl-certs"
              hostPath:
                path: "/usr/share/ca-certificates"
            - name: "kubeconfig"
              hostPath:
                path: "/etc/kubernetes/worker-kubeconfig.yaml"
            - name: "etc-kube-ssl"
              hostPath:
                path: "/etc/kubernetes/ssl"
            - hostPath:
                path: /var/run/dbus
              name: dbus

  - path: /etc/kubernetes/worker-kubeconfig.yaml
    content: |
        apiVersion: v1
        kind: Config
        clusters:
        - name: local
          cluster:
            certificate-authority: /etc/kubernetes/ssl/ca.pem
        users:
        - name: kubelet
          user:
            client-certificate: /etc/kubernetes/ssl/worker.pem
            client-key: /etc/kubernetes/ssl/worker-key.pem
        contexts:
        - context:
            cluster: local
            user: kubelet
          name: kubelet-context
        current-context: kubelet-context

{{ if not .UseCalico }}
  - path: /etc/kubernetes/cni/net.d/10-flannel.conf
    content: |
        {
            "name": "podnet",
            "type": "flannel",
            "delegate": {
                "isDefaultGateway": true
            }
        }
{{ end }}
